{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4e3b9-f2ce-4b76-90e1-11088c64885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # Visualization\n",
    "import plotly.express as px # Visualization\n",
    "import seaborn as sns # Visualization\n",
    "import pickle # Save model files\n",
    "\n",
    "# Libraries for model building\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input\n",
    "\n",
    "# For Evalution we will use these library\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score \n",
    "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f910298-730a-4503-9dd0-69b43443d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv file\n",
    "df = pd.read_csv('/Users/kittipot/Desktop/Bitcoin_data/btcusd_1-min_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1aafc-8331-496d-b430-a6628ed90ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exploring data analysis in dataframe\n",
    "\n",
    "# Check Head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Check missing value in data\n",
    "print(df.isnull().sum())\n",
    "# check data info\n",
    "print(df.info())\n",
    "\n",
    "# Convert unix timestamp To datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# Convert to Y-m-d format\n",
    "df['Timestamp'] = df['Timestamp'].dt.strftime('%Y-%m-%d')\n",
    "print(df['Timestamp'])\n",
    "\n",
    "# Set timestamp Column to be index\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Convert Scientific Notation output to Float\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Visualize data stat\n",
    "print((df.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17dd29a-9145-4bf0-a3bb-f2ee858ed9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw box plot To Visualize distribution in each columns\n",
    "cols = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "for col in cols.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=cols[col])\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f3e3e-668a-43ed-a1fe-9af96e570b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_2 = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "# Create histogram to Visualize distribution in each column\n",
    "for col in cols_2.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(cols_2[col], kde=True, bins=100, color='blue')  # histogram + density\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5c133-9473-46bf-b88f-923c28b3d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In This Project We Will Use Data Between 2021-2022 To Predict Bitcoin Price In 2023\n",
    "\n",
    "# Split Data Into train & test data\n",
    "train_data = df['2021-01-01':'2022-12-31']  # Training data\n",
    "test_data = df['2023-01-01': '2023-12-31']  # Testing data\n",
    "\n",
    "# Inspect Data Type\n",
    "print(f\"Training Data Shape: {train_data.shape}\")\n",
    "print(f\"Testing Data Shape: {test_data.shape}\")\n",
    "\n",
    "# Normalize Data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_data[['Open', 'High', 'Low', 'Close',\n",
    "                                    'Volume']])  # Fit Only Training Set \n",
    "test_scaled = scaler.transform(test_data[['Open', 'High', 'Low', 'Close',\n",
    "                                    'Volume']])\n",
    "# Inspect Output\n",
    "print(train_scaled.shape)\n",
    "print(test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5805d-ead5-48e8-a23a-0505c242f4aa",
   "metadata": {},
   "source": [
    "การ fit เฉพาะชุดข้อมูล training set (ชุดข้อมูลที่ใช้ฝึกโมเดล) ในกระบวนการทำ Normalization หรือ Scaling มีเหตุผลสำคัญด้านความถูกต้องของการวิเคราะห์และการป้องกัน data leakage ดังนี้:1. ป้องกัน Data Leakage\n",
    "การ fit กับทั้งชุดข้อมูล (รวมทั้ง training และ test set) อาจทำให้ข้อมูลจาก test set ซึ่งควรถูกใช้เพียงเพื่อการประเมินผลลัพธ์ของโมเดล ถูกเปิดเผยระหว่างการฝึกโมเดล\n",
    "การเปิดเผยข้อมูลล่วงหน้า (data leakage) ทำให้โมเดลได้รับข้อมูลที่ไม่ควรรู้ก่อนการทดสอบ ซึ่งส่งผลต่อความแม่นยำที่ไม่สะท้อนความสามารถที่แท้จริงของโมเดลในสถานการณ์จริง\n",
    "2. จำลองสถานการณ์จริง\n",
    "ในการใช้งานจริง โมเดลจะต้องประมวลผลข้อมูลใหม่ที่ไม่เคยเห็นมาก่อน (เหมือน test set) โดยใช้ค่าพารามิเตอร์ที่ได้จากข้อมูลในอดีต (training set)\n",
    "การ fit เฉพาะ training set แล้วใช้ค่าพารามิเตอร์ที่ได้ (เช่น ค่า min และ max จาก MinMaxScaler) ไป transform test set จึงเลียนแบบสถานการณ์จริงได้ดีที่สุด\n",
    "3. สอดคล้องกับแนวคิด Machine Learning Pipeline\n",
    "การฝึก (training) และการทดสอบ (testing) เป็นขั้นตอนที่แยกจากกันโดยชัดเจน\n",
    "พารามิเตอร์ที่ได้จากการปรับสเกล (scaling) เช่น ค่า min และ max หรือค่าเฉลี่ย (mean) และส่วนเบี่ยงเบนมาตรฐาน (std) ควรมาจาก training set เท่านั้น เพื่อให้ pipeline ถูกต้อง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f833cdb-33cd-4bbe-819d-1fc12ac3fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Data To DataFrame\n",
    "train_scaled = pd.DataFrame(train_scaled, columns=train_data.columns, index=train_data.index)\n",
    "test_scaled = pd.DataFrame(test_scaled, columns=test_data.columns, index=test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698d286-1304-411b-b0a2-289dc71b64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled_col = train_scaled.iloc[:, 0:5]\n",
    "\n",
    "# Visualize distribution in each column after normalization\n",
    "for col in train_scaled_col.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(train_scaled_col[col], kde=True, bins=100, color='blue')  # histogram + density\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6924c-4a07-4e27-ac89-f7a8e0ed3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_col = test_scaled.iloc[:, 0:5]\n",
    "\n",
    "# Visualize distribution in each column\n",
    "for col in test_scaled_col.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(test_scaled_col[col], kde=True, bins=100, color='blue')  # histogram + density\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38e5a3-dd7d-4fb2-be99-3f5d4af9e1d4",
   "metadata": {},
   "source": [
    "ต่อไป เราจะเขียนฟังก์ชัน create sliding_window function เพื่อแปลงข้อมูลให้เป็น3มิติเพื่อเตรียมใช้กับ LSTM Model\n",
    "ตัวแปร X เป็น Input สำหรับโมเดล ซึ่งประกอบด้วยช่วงข้อมูลในอดีตตามจำนวน timesteps ที่กำหนด\n",
    "แต่ละรายการใน X มีลักษณะเป็นกลุ่มข้อมูลที่มีความยาว timesteps และข้อมูลในทุกคอลัมน์ของ data\n",
    "ตัวแปร y เป็น Target (Output) ที่โมเดลต้องทำนาย\n",
    "สำหรับแต่ละรายการใน X ค่าใน y จะเป็นข้อมูลในคอลัมน์ที่ระบุด้วย target_col_idx ของตำแหน่งถัดไปจากช่วงข้อมูลใน X\n",
    "ส่งค่ากลับไปในตัวแปร X, y \n",
    "X: sliding window สำหรับ features\n",
    "y: ค่าTarget สำหรับแต่ละ window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718fa492-471b-46fd-abf2-8b94e3c0c180",
   "metadata": {},
   "source": [
    "ในที่นี้เราจะใช้ ['Close'] ทั้งใน Features และ Target เนื่องจาก\n",
    "1.Time Series Forecasting: ต้องการพยากรณ์ราคาปิด (Close) ของช่วงเวลาถัดไป โดยอิงจากข้อมูลราคา Close ในอดีต\n",
    "2.กรณีโมเดลได้รับลำดับเวลา (sequence): เช่น การใช้ LSTM หรือ RNN ที่สามารถจับความสัมพันธ์ของข้อมูลในอดีตได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ea5e4-5ec4-4891-a060-29415e677401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Function To Transform Data\n",
    "\n",
    "def create_sliding_window(data, target_col_idx, timesteps):\n",
    "    # Create Input X using numpy indexing\n",
    "    X = np.array([data[i:i + timesteps, :] for i in range(len(data) - timesteps)])\n",
    "    # Create Output y using target_col_idx\n",
    "    y = np.array([data[i + timesteps, target_col_idx] for i in range(len(data) - timesteps)])\n",
    "    return X, y\n",
    "\n",
    "# Transform DataFrame To numpy array\n",
    "train_array = train_scaled.values\n",
    "test_array = test_scaled.values\n",
    "\n",
    "# Assign Timestep\n",
    "timesteps = 30\n",
    "\n",
    "# Assign index from Target column\n",
    "target_col_idx = train_scaled.columns.get_loc('Close')\n",
    "\n",
    "# Create Sliding Window\n",
    "X_train, y_train = create_sliding_window(train_array, target_col_idx, timesteps)\n",
    "X_test, y_test = create_sliding_window(test_array, target_col_idx, timesteps)\n",
    "\n",
    "# Inspect Data Shape\n",
    "print(\"X_train shape:\", X_train.shape)  # (samples, timesteps, features)\n",
    "print(\"y_train shape:\", y_train.shape)  # (samples,)\n",
    "print(\"X_test shape:\", X_test.shape)    # (samples, timesteps, features)\n",
    "print(\"y_test shape:\", y_test.shape)    # (samples,)\n",
    "\n",
    "# Inspect Sliding Window index 10\n",
    "window_idx = 10\n",
    "print(f\"X_train[{window_idx}]:\\n\", X_train[window_idx])\n",
    "print(f\"y_train[{window_idx}]:\", y_train[window_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49051c6-107c-47e3-84cf-ff331490510a",
   "metadata": {},
   "source": [
    "window_idx = 10\n",
    "print(f\"X_train[{window_idx}]:\\n\", X_train[window_idx])\n",
    "print(f\"y_train[{window_idx}]:\", y_train[window_idx])\n",
    "\n",
    "แสดงข้อมูล(Feature)ใน sliding window ที่ 10 (X_train[10]) และค่าTargetที่สอดคล้อง (y_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1215e7-baa0-4d25-90de-75bb5724377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect timesteps and features From X_train\n",
    "timesteps = X_train.shape[1]  # Amount of timestep\n",
    "features = X_train.shape[2]   # Amount of features\n",
    "print(\"Timesteps:\", timesteps)\n",
    "print(\"Features:\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d69c5-9b20-4824-9775-82ccbe091686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "model=Sequential()\n",
    "\n",
    "# Add LSTM Layer\n",
    "model.add(Input(shape=(timesteps, features)))\n",
    "model.add(LSTM(64, activation=\"relu\"))\n",
    "\n",
    "# Add Dense layer For Forecast Output (Ex. Close Price)\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=3,batch_size=32,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10604aa-4e32-4dc5-834c-8f6ecdda7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model that already train\n",
    "model.save('/Users/kittipot/Desktop/Machine Learning Files/LSTM_Bitcoins_Predict.keras')  # Save as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c33f75-3e68-41ad-9d7b-69c366ca9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = load_model('/Users/kittipot/Desktop/Machine Learning Files/LSTM_Bitcoins_Predict.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c213e1b-9fbb-477e-8617-a346145a7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model loss/validation loss to use next time\n",
    "# Save history data\n",
    "history_path = '/Users/kittipot/Desktop/Machine Learning Files/history.pkl'\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b021a-6d2d-4aa1-b290-c1bf6a96e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model loss/validation loss\n",
    "# Load history data\n",
    "history_path = '/Users/kittipot/Desktop/Machine Learning Files/history.pkl'\n",
    "with open(history_path, 'rb') as f:\n",
    "    loaded_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77feab96-e4b6-410d-9b02-125a6ac92e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training loss VS Validation loss During Train Process\n",
    "\n",
    "\"\"\"loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\"\"\"\n",
    "\n",
    "loss = loaded_history['loss'] # In case of loading the history file, use this code\n",
    "val_loss = loaded_history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ee444-4a36-4fe4-8f84-cc63c5983bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Do the prediction and check performance metrics\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "train_predict.shape, test_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d035a6-5c75-484a-b81d-f3bc72d0f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_predict)\n",
    "print(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b4ac3-024e-4b92-8c26-7ab3367c5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf581d-ed80-4c29-a5dc-742294d9328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler For Target\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# Transform Target\n",
    "scaled_target = scaler_target.fit_transform(train_data[['Close']])  # Keep transform data in scaled_target variable \n",
    "\n",
    "# Transform back to original form\n",
    "train_predict = scaler_target.inverse_transform(train_predict)\n",
    "test_predict = scaler_target.inverse_transform(test_predict)\n",
    "original_ytrain = scaler_target.inverse_transform(y_train.reshape(-1, 1)) \n",
    "original_ytest = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print(\"Transformed Back to Original Scale!\")\n",
    "print(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b918ddee-80fe-428d-8ee9-af1003791898",
   "metadata": {},
   "source": [
    "เราต้องแยกตัวแปรสำหรับข้อมูลที่ถูกสเกล (scaled_target) และตัวแปรที่เป็น MinMaxScaler (scaler_target) ออกจากกัน เพื่อให้ scaler_target ยังคงเป็นตัวแปรที่เก็บออบเจ็กต์ MinMaxScaler ไว้สำหรับการใช้งาน inverse_transform ในภายหลัง\n",
    "\n",
    "ตัวแปร scaler_target:\n",
    "เก็บวัตถุ MinMaxScaler ที่ผ่านการเรียนรู้ขอบเขต (min, max) ของข้อมูลใน train_data[['Close']] เพื่อใช้งาน transform และ inverse_transform\n",
    "ตัวแปร scaled_target:\n",
    "เก็บข้อมูลที่ถูกสเกลไว้สำหรับการใช้งานในภายหลัง (เช่น การฝึกโมเดล) โดยไม่กระทบกับการใช้งาน scaler_target\n",
    "\n",
    "การ fit ด้วยข้อมูล test อาจทำให้เกิดปัญหาข้อมูลรั่วไหล (data leakage) ซึ่งส่งผลต่อความน่าเชื่อถือของการประเมินโมเดล.\n",
    "\n",
    "ใช้ fit เฉพาะกับ train data:\n",
    "ให้แน่ใจว่า MinMaxScaler ถูก fit ด้วยข้อมูลชุด train เท่านั้น และใช้ transform กับข้อมูลชุด test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21531403-26e3-4510-9783-baa78aa642e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrices RMSE, MSE and MAE\n",
    "print(\"Train data RMSE: \", np.sqrt(mean_squared_error(original_ytrain,train_predict)))\n",
    "print(\"Train data MSE: \", mean_squared_error(original_ytrain,train_predict))\n",
    "print(\"Train data MAE: \", mean_absolute_error(original_ytrain,train_predict))\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Test data RMSE: \", np.sqrt(mean_squared_error(original_ytest,test_predict)))\n",
    "print(\"Test data MSE: \", mean_squared_error(original_ytest,test_predict))\n",
    "print(\"Test data MAE: \", mean_absolute_error(original_ytest,test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf0e7e-6de6-4f2c-bd97-8e6ee8079341",
   "metadata": {},
   "source": [
    "ต่อไปเราจะทำการเปรียบเทียบราคาปิดจริงกับราคาปิดที่โมเดลทำนายได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650b1d2-596d-47f0-a8bd-3e41add6514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect length of each \n",
    "print(\"Length of train_predict:\", len(train_predict.flatten()))\n",
    "print(\"Length of test_predict:\", len(test_predict.flatten()))\n",
    "print(\"Length of train_data:\", len(train_data))\n",
    "print(\"Length of test_data:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107429f0-cebf-4f3b-9ce0-1a1420b9d316",
   "metadata": {},
   "source": [
    "1.เหตุผลที่ข้อมูลเหลื่อมกัน:\n",
    "ข้อมูล train_predict และ test_predict จะเริ่มต้นหลังจาก timesteps ตัวแรกของ train_data และ test_data เพราะข้อมูลเริ่มต้น timesteps ถูกใช้เพื่อสร้าง input sequence (X_train และ X_test) และไม่มีการทำนายสำหรับตำแหน่งเหล่านั้น\n",
    "ดังนั้น train_predict และ test_predict จะมีขนาดน้อยกว่าข้อมูลต้นฉบับ (train_data และ test_data) เท่ากับจำนวน timesteps ที่คุณกำหนดไว้\n",
    "2.การแก้ปัญหา: เราต้องปรับข้อมูล train_data และ test_data ให้สอดคล้องกับ train_predict และ test_predict โดยลบ timesteps แรกออกจาก train_data และ test_data ก่อนการเปรียบเทียบ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace70d0-d9bc-4e0e-9cf3-3bdaa2e276eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust test_data according timesteps\n",
    "adjusted_test_data = test_data.iloc[timesteps:]\n",
    "\n",
    "# inspect interest data range\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# filter data according to interest data range\n",
    "focused_test_data = adjusted_test_data.loc[start_date:end_date]\n",
    "\n",
    "# Create DataFrame\n",
    "focused_plotdf = pd.DataFrame({\n",
    "    'date': (focused_test_data.index.to_series()),\n",
    "    'original_close': (focused_test_data['Close']),\n",
    "    'predicted_close': (test_predict[:len(focused_test_data)].flatten()\n",
    "    )\n",
    "})\n",
    "\n",
    "# Create graph to comparison\n",
    "fig = px.line(\n",
    "    focused_plotdf,\n",
    "    x='date',\n",
    "    y=['original_close', 'predicted_close'],\n",
    "    labels={'value': 'Stock price', 'date': 'Date'}\n",
    ")\n",
    "fig.update_layout(\n",
    "    title_text='Comparison of Original Close Price vs Predicted Close Price (Focused)',\n",
    "    plot_bgcolor='white',\n",
    "    font_size=15,\n",
    "    font_color='black',\n",
    "    legend_title_text='Close Price'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401de24d-8bf7-4bb4-9a3f-03e9441e6475",
   "metadata": {},
   "source": [
    "1.ฟังก์ชัน np.concatenate ใช้ในการรวมอาเรย์ (array) หลาย ๆ อันเข้าด้วยกันตามแกนที่กำหนด (ค่าเริ่มต้นคือแกน 0)\n",
    "2.train_predict[:len(focused_train_data)]\n",
    "เป็นการตัดข้อมูลในอาเรย์ train_predict ให้มีความยาวเท่ากับ focused_train_data โดยใช้ slicing:\n",
    "[:len(focused_train_data)] หมายถึงเลือกเฉพาะตำแหน่งตั้งแต่ต้นจนถึงตำแหน่งที่ len(focused_train_data)\n",
    "3.flatten() ใช้แปลงอาเรย์ที่มีหลายมิติให้กลายเป็นอาเรย์แบบ 1 มิติ (flat array)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d76da68-13ee-46a5-9072-3f997a819c6b",
   "metadata": {},
   "source": [
    "หากต้องการใช้โมเดลที่เทรนจากข้อมูลปี 2021-2022 และคาดการณ์ข้อมูลสำหรับปี 2023 แล้ว ในการทำนาย 30 วันข้างหน้า (เช่น วันที่ 1-30 มกราคม 2024) สามารถใช้วิธีการ forecasting (การทำนายล่วงหน้า) สำหรับชุดข้อมูลที่ยังไม่ได้เห็น โดยไม่จำเป็นต้องมีข้อมูลจากปี 2024 ในตอนแรก โดยมีวิธีการคือ:\n",
    "\n",
    "1. เตรียมข้อมูลจากปี 2023 สำหรับทำนายในปี 2024\n",
    "เนื่องจากเราต้องการทำนายราคาในอนาคต (30 วันข้างหน้า) เราสามารถเริ่มต้นจากราคาปิดล่าสุดของปี 2023 (เช่น ราคาปิดวันที่ 31 ธันวาคม 2023) หรือราคาของช่วงวันสุดท้ายในชุดข้อมูลทดสอบ (test data) เพื่อใช้เป็นจุดเริ่มต้นในการทำนายล่วงหน้า\n",
    "\n",
    "ดังนั้นเราจะต้องเตรียม input features สำหรับการทำนายใน 30 วันข้างหน้า โดยโมเดล LSTM หรือ RNN มักจะใช้ข้อมูลล่าสุดเพื่อทำนายราคาต่อไป (เช่น ค่าของราคาปิดใน 30 วันล่าสุด)\n",
    "\n",
    "2. ทำนาย 30 วันข้างหน้า (Forecasting)\n",
    "การทำนาย 30 วันข้างหน้าจะใช้ข้อมูลราคาปิดของช่วงก่อนหน้า เพื่อให้โมเดลทำนายราคาปิดวันถัดไป\n",
    "\n",
    "วิธีการใช้โมเดลทำนายราคาต่อเนื่อง\n",
    "\n",
    "โมเดลที่เราฝึกแล้วสามารถทำนายได้ทีละขั้น เช่น\n",
    "\n",
    "ใช้ข้อมูลราคาปิดล่าสุด (วันที่ 31 ธันวาคม 2023 หรือข้อมูลสุดท้ายใน test data) เพื่อทำนายราคาในวันที่ 1 มกราคม 2024\n",
    "ใช้ผลทำนายที่ได้จากวันที่ 1 มกราคม 2024 เป็นข้อมูลอินพุตในการทำนายราคาของวันที่ 2 มกราคม 2024\n",
    "ทำเช่นนี้จนกระทั่งทำนายครบ 30 วัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4d808-da40-4989-a4d0-86892cf92d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last 30th days data from test_data (according timesteps we use to train model)\n",
    "last_known_data = test_data[['Close', 'Open', 'High', 'Low', 'Volume']].iloc[-30:].values # Convert dataframe to array\n",
    "last_known_data = last_known_data.reshape((1, 30, 5))  # reshape into [batch_size, time_steps, features]\n",
    "\n",
    "# Create list to keep prediction result\n",
    "predictions = []\n",
    "\n",
    "for i in range(30):  # predict next 30th days\n",
    "    # Predict next day close price\n",
    "    predicted_value = model.predict(last_known_data)\n",
    "    \n",
    "    # keep prediction result\n",
    "    predictions.append(predicted_value[0][0])\n",
    "    \n",
    "    # Update latest known data (Use predict value to be input data for predict next day price)\n",
    "    # Shift the data by 1 day and insert the newly predicted value in the last position.\n",
    "    new_input = np.append(last_known_data[:, 1:, :], np.reshape([[predicted_value[0][0], predicted_value[0][0], predicted_value[0][0], predicted_value[0][0], 0]], (1, 1, 5)), axis=1)\n",
    "    last_known_data = new_input\n",
    "\n",
    "# Convert Scientific Notation output to Float\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Visualize next 30th days predictions\n",
    "predicted_dates = pd.date_range(start=\"2024-01-01\", periods=30, freq='D')\n",
    "forecast_df = pd.DataFrame({\n",
    "    'date': predicted_dates,\n",
    "    'predicted_close': predictions\n",
    "})\n",
    "\n",
    "# Show output\n",
    "print(forecast_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b65216e-63a0-49a9-9a20-8defe63107db",
   "metadata": {},
   "source": [
    "predicted_value[0][0] คือการเข้าถึงค่าที่ทำนายจากผลลัพธ์ที่ได้จาก model.predict():\n",
    "predicted_value[0] หมายถึงแถวแรก (batch แรก) ของผลลัพธ์\n",
    "predicted_value[0][0] คือค่าที่ทำนายในตำแหน่งแรก (ตัวเดียวในกรณีนี้ ซึ่งอาจเป็นราคา Close ของวันถัดไป)\n",
    "ค่าที่ได้จะถูกเพิ่มเข้าไปในลิสต์ predictions โดยใช้ append(). ค่าที่เก็บนี้จะเป็นผลลัพธ์ที่ทำนายจากโมเดลสำหรับวันถัดไป\n",
    "\n",
    "ภาพรวมของกระบวนการเลื่อนข้อมูล\n",
    "เลื่อนข้อมูล 1 วัน: การใช้ [:, 1:, :] เลื่อนไป 1 วันเพื่อให้ข้อมูลที่เก่าที่สุดถูกลบออก (ข้อมูลวันแรกในชุด) และข้อมูลใหม่ที่ทำนายจะถูกใส่แทนในตำแหน่งสุดท้าย.\n",
    "เพิ่มข้อมูลที่ทำนาย: ค่าที่ทำนาย (predicted_value) จะถูกแทรกเป็นข้อมูลล่าสุดในตำแหน่งสุดท้ายของ last_known_data สำหรับใช้ในการทำนายในรอบถัดไป.\n",
    "การอัปเดตข้อมูล: ข้อมูล last_known_data จะถูกอัปเดตให้พร้อมสำหรับการทำนายในรอบถัดไป."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
